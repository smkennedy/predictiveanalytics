---
title: "Predictive Analytics - Excercises 1"
author: "Shane Kennedy"
date: "Friday, August 07, 2015"
output: md_document
---

##Exploratory Analysis - Voting Data


```{r, warning = FALSE, echo=FALSE}
##Reading in data:
g2 = read.csv("https://raw.githubusercontent.com/jgscott/STA380/master/data/georgia2000.csv")


```

**Do certain kinds of voting equipment lead to higher rates of undercount?**

Summarizing the data into the table below allows for easy comparison between the relative rates of undercount for each of the equipment types. It's clear that punch cards stand out as having a higher rate of undercount relative to the other methods while optical equipment resulted in the lowest rate of undercount. The absolute number for undercount is also an important consideration. Optical and punch card equipment is used with a much larger percentage of the total population, so a higher relative undercount with these methods will result in a larger effect on the absolute number of votes counted.

```{r, warning = FALSE, echo= FALSE}
##Calculation of absolute and percentage undercount:
library(plyr)
summary = ddply(g2, .(equip), summarise, sum = sum(ballots))
colnames(summary)[2] = "ballots"
summary$votes = ddply(g2, .(equip), summarise, sum = sum(votes))[,2]
summary$undercount = (summary$ballots - summary$votes)
summary$percentundercount = (summary$ballots - summary$votes)/(summary$votes)

summary
```

**Does a higher rate of undercounting for certain types of equipment (i.e. lever and punch card) result in a disparate impact on poor and minority communities?**

The plots below provide insight on whether the difference between the rate of undercount disparately affects minority or poor populations. 


```{r, echo=FALSE}

plot(g2$equip, g2$perAA, main = "Voting Equip. & Demographics", xlab = "Equipment Type", ylab = "County Percent African American")

```


Counties that use paper voting, one of the equipment types with a lower rate of undercount, have the highest average African American popluation. However, it is important to note that only 3,454 ballots were made via paper, a number that is fairly insignificant in relation to the 2,691,314 ballots submitted.  

With this consideration, to analyze whether there was a disparate impact on minorities, the next question is how the popluations of lever and punch card voting compare to optical (as shown in the summary table I created above, lever and punch card equipment had a much higher rate of undercount than optical equipment). In the box plot, it's clear that the counties that use either lever or punch card equipment typically have significantly higher African American populations than counties that use optical equipment. As a result, it can be concluded that there was a disparate effect on African American populations.

Looking at relative poverty between counties using different equipment types reveals a similar conclusion:


```{r, echo=FALSE}
g2$poor = as.factor(g2$poor)
par(xpd=NA, mar=c(5,4,4,6.5)+0.1, yaxt="n")
barplot(prop.table(table(g2$poor,g2$equip),2), ylab = "Percent of Counties", xlab = "Equipment Type", col=c("white","grey"), main = "Poverty by Equip. Type")
legend(5,.6,legend= "Poor", lty=c("solid"), col=c("grey"), bty="n", cex=0.8)
```

The poorer populations were clearly more adversely affected by undercounting. Optical has by far the fewest counties meeting the povery cut-off defined in the dataset (where >25% of the population must live below 1.5 * the Federal poverty line) and it had the lowest rate of undercount. Counties that used paper, lever, or punch card equipment were more likely to be considered poor and faced higher rates of undercount.

#Bootstrapping - ETF Data

For this analysis, I pulled price data and calculated daily returns for the period 8/1/2010 to 7/31/2015.

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(fImport)
library(foreach)
library(mosaic)
etfs = c("SPY","TLT","LQD","EEM","VNQ")
myprices = yahooSeries(etfs, from='2010-08-01', to='2015-07-31')
YahooPricesToReturns = function(series) {
  mycols = grep('Adj.Close', colnames(series))
	closingprice = series[,mycols]
	N = nrow(closingprice)
	percentreturn = as.data.frame(closingprice[2:N,]) / as.data.frame(closingprice[1:(N-1),]) - 1
	mynames = strsplit(colnames(percentreturn), '.', fixed=TRUE)
	mynames = lapply(mynames, function(x) return(paste0(x[1], ".PctRet")))
	colnames(percentreturn) = mynames
	as.matrix(na.omit(percentreturn))
}

myreturns = as.data.frame(YahooPricesToReturns(myprices))
```

To analyze the relative returns and risk for each of the ETFs over the time period, I calculated the standard deviation, mean, and range of returns below:

**Standard Deviation:**
```{r, message = FALSE, warning = FALSE, echo = FALSE}
sort(apply(myreturns, 2, sd))
```

**Mean:**
```{r, message = FALSE, warning = FALSE, echo = FALSE}
sort(apply(myreturns, 2, mean))
```

**Range:**
```{r, message = FALSE, warning = FALSE, echo = FALSE}
apply(myreturns, 2, range)

```

The standard deviations and means have been sorted from lowest to highest in the tables above (leftmost is least volatile/least reward).

In most cases, the asset classes with higher volatility seem to provide higher average daily returns. This intuitively makes sense--as investors take on more risk, they expect a higher return. Exceptions to this are SPY, which ranked higher for returns than its volatility rank, and EEM, which ranked lowest for returns and which was the most volatile. While the risk/reward trade-off between standard deviation and returns is an easy way of looking at assets, there is clearly more to consider.

In particular, some of these ETFs may have return distributions with "fatter" tails and they may experience more extreme events than expected by a standard gaussian distribution. It's easy to see hints of this when looking at TLT versus SPY. While TLT has a higher standard deviation than SPY, SPY has a much wider range and the asset class may experience extreme events more frequently. 

A simple way of viewing this graphically through a boxplot. Here, we can see that there are many outliers. Specifically, SPY appears to experience far more extreme events than TLT, even though the latter has higher volatility. Between all asset classes, LQD seems to have the tighest grouping of returns.

```{r, echo = FALSE}
boxplot(myreturns, main = "ETF Returns & Outliers", ylab = "Daily Return")
```

Another view of the distributions can be seen through the histograms below:

```{r, echo = FALSE}
par(mfrow=c(1,2))
hist(myreturns$SPY.PctRet, main = "SPY - Freq. of Returns", ylab = "Frequency", xlab = "Daily Return")
hist(myreturns$TLT.PctRet, main = "TLT - Freq. of Returns", ylab = "Frequency", xlab = "Daily Return")
hist(myreturns$LQD.PctRet, main = "LQD - Freq. of Returns", ylab = "Frequency", xlab = "Daily Return")
hist(myreturns$EEM.PctRet, main = "EEM - Freq. of Returns", ylab = "Frequency", xlab = "Daily Return")
hist(myreturns$VNQ.PctRet, main = "VNQ - Freq. of Returns", ylab = "Frequency", xlab = "Daily Return")
```


While it's clear there is a well defined mean in the returns it's also clear that the distributions we are dealing with are not completely normal. The frequency of outliers in the histograms again indicate the presence of "fat tails", or positive kurtosis.

With the volatilities, tail considerations, and distributions of negative returns above, it seems safe to classify LQD as the safest asset class by far. On the opposite end of the spectrum, EEM and VNQ are by far the most risky given the very high standard deviations and large number of outliers on the downside--hinting that the distributions may have tails that are particularly fat. Between SPY and TLT, even though SPY has a slightly higher standard deviation, TLT seems to be less risky as its distribution appears to be tighter and it has experienced fewer outliers on the negative end of the spectrum.

Before construting a safe and risky portfolio, one final consideration is the correlation between asset classes.

**Correlation Matrix:**
```{r, echo=FALSE}
cor(myreturns)
```

The matrix shows that equities tend to move with equities and bonds tend to move with bonds. For example, SPY and EEM, two equity ETFs, have a fairly high positive correlation to one another while these ETFs are both negatively correlated to TLT and LQD, the two bond products. When creating a portfolio, this is a key consideration because a negative correlation implies that ETFs are less likely to fall in value together. This is the basic idea behind holding a diversified portfolio.

With that in mind, I have defined my portfolios and run VaR analysis for a 20 day holding period as follows:

Simulation 1 - "Base": 20% in each asset class


```{r, echo = FALSE}
library(mosaic)
library(foreach)

set.seed(5)
n_days = 20
sim1 = foreach(i=1:20, .combine='rbind') %do% {
  totalwealth = 100000
	weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
	holdings = weights * totalwealth
	wealthtrackerbase = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(myreturns, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		totalwealth = sum(holdings)
		wealthtrackerbase[today] = totalwealth
	}
	wealthtrackerbase
}

## VaR
basevar = quantile(sim1[,n_days], 0.05) - 100000
```

Simulation 2 - "Safe": 30% SPY, 30% TLT, 40% LQD *(lowest standard deviations, relatively few outliers, SPY and TLT/LQD are negatively correlated)*

```{r, echo = FALSE}
set.seed(5)
sim2 = foreach(i=1:20, .combine='rbind') %do% {
  totalwealth = 100000
  weights = c(0.3, 0.3, 0.4, 0.0, 0.0)
	holdings = weights * totalwealth
	wealthtrackersafe = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(myreturns, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		totalwealth = sum(holdings)
		wealthtrackersafe[today] = totalwealth
	}
	wealthtrackersafe
}

##VaR

safevar = quantile(sim2[,n_days], 0.05) - 100000

```


Simulation 3 - "Risky": 50% EEM, 50% VNQ *(high standard deviations, many outliers, and highly correlated)*

```{r, echo = FALSE}
set.seed(5)
sim3 = foreach(i=1:20, .combine='rbind') %do% {
  totalwealth = 100000
  weights = c(0.0, 0.0, 0.0, 0.5, 0.5)
  holdings = weights * totalwealth
	wealthtrackerrisky = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(myreturns, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		totalwealth = sum(holdings)
		wealthtrackerrisky[today] = totalwealth
	}
	wealthtrackerrisky
}

##VaR:

riskyvar = quantile(sim3[,n_days], 0.05) - 100000


```

**5% Value at Risk by portfolio (20 day):**
```{r, echo = FALSE}
VaRs = data.frame(safevar,basevar,riskyvar)

VaRs
```

These value at risk numbers show the dollar amount an investor could expect to lose with 5% probability over a four week period in each portfolio. Clearly--investors in the risky portfolio must be willing to accept larger downside risk.

That said, investors would not be willing to take such risks without the possibility for greater reward. 

To illustrate this, I have calculated the 75th and 95th percentile returns for each of the portfolio distributions:

**75th Percentile Total Portfolio Value (20 day):**
```{r, echo = FALSE}
percseventyfive = data.frame(quantile(sim1[,n_days], 0.75),quantile(sim2[,n_days], 0.75),quantile(sim3[,n_days], 0.75))
colnames(percseventyfive) = c("Base","Safe","Risky")
percseventyfive
```                          

**95th Percentile Total Portfolio Value (20 day):**
```{r, echo=FALSE}
percninetyfive = data.frame(quantile(sim1[,n_days], 0.95),quantile(sim2[,n_days], 0.95),quantile(sim3[,n_days], 0.95))
colnames(percninetyfive) = c("Base","Safe","Risky")
percninetyfive
```                        

Clearly, with riskier portfolios there is also the possibility for a higher payout on the upside. In choosing between these three portfolios, investors should consider their goals and whether they can take on additional downside risk for a potentially larger payout. 
   
   
#Clustering and PCA
```{r, echo = FALSE}
wine = read.csv("https://raw.githubusercontent.com/jgscott/STA380/master/data/wine.csv")
```

**Clustering**

For clustering the wine data, k-means makes more sense than hierarchical because the data set is fairly large and it may not make sense to group chemical components in a hierarchical tree. While it's certainly possible to create such a tree, splitting on chemical properties doesn't seem to make as much intuitive sense as grouping based on chemical similarity. Wines are often grouped into many categories based on resembelance to each other--which seems more analogous to the k-means process. I also tried running hierarchical trees and that resulted in a tree without a clear height to cut at.

I ran the k-means analysis with five clusters. The properties of these clusters can be seen below:

```{r, echo = FALSE}
wine_scaled = as.data.frame(scale(wine[,1:12], center = TRUE, scale = TRUE))
wine_scaled$color = wine$color
set.seed(3)
wine_dist = dist(wine_scaled[1:11], method = 'euclidean')
kmeans = kmeans(wine[1:11], centers = 5)
wine$cluster = as.factor(kmeans$cluster)
kmeans$center
```

**Principal Component Analysis**

Running principal component analysis on this same data gave the variance plot below:

```{r, echo = FALSE}
pc1 = prcomp(wine_scaled[,1:11])
plot(pc1, main = "Principal Component Analysis", xlab = "Principal Components")

```

After centering and scaling the chemical properties, principal component analysis explains a moderate amount of variance between the first few components but much of the variance still remains. This is easy to see in the sumary of the PCA below:

Summary of PCA:
```{r, echo = FALSE}
summary(pc1)

```

To explain 90% of the variance in the data set, a full 7 principal components would need to be used.

Given that there are just 11 chemical dimensions, this indicates that the data structure may not have a clear orientation and PCA is probably not the best method to use here.

**Comparison of Methods**

With the above considerations, clustering seems to make more sense for this data. While the orientation of the points could be clearer in PCA, wines can usually be grouped into distinct categories (sweet white, dry red, etc.) which share chemical properties and this fact seems to lends itself well to clustering.

**Evaluation of Clustering**

The plot below shows how well clustering was able to differentiate between two of the clearest categories for wine--white and red:


```{r, echo = FALSE}
par(xpd=NA, mar=c(5,4,4,6.5)+0.1, yaxt="n")
plot(wine$cluster, wine$color, xlab = "Cluster", main = "Color by Cluster", col = c("black","grey"), ylab = "Percent of Cluster")
legend(1.08,.6,legend=c("Red", "White"), lty=c("solid","solid"), col=c("black","grey"), bty="n", cex=0.8, title = "Color")
```


Clustering does seem to be fairly effective in splitting on wine color. Cluster 1 is heavily weighted to red wine, while 3, 4, and 5, are almost completely white. The only cluster with some ambiguity is cluster 2. This cluster seems to represent wines that are similar in chemical properties across color. Perhaps these are wines that lie somewhere in between the typical red/white spectrum (maybe Rose?).

We can also evaluate how successful wine is in grouping quality wines together:

```{r, echo = FALSE}
par(xpd=NA, mar=c(5,4,4,6.5)+0.1, yaxt="n")
plot(wine$cluster, as.factor(wine$quality), ylab = "Percent of Cluster", xlab = "Cluster", main = "Quality by Cluster", yaxt="n")
greys = grey.colors(7, start = 0.1, end = 0.7) 
legend(1.1,.6,legend= levels(as.factor(wine$quality)), lty=c("solid"), col=greys, bty="n", cex=0.8, title = "Quality")

```

Clearly, clustering is very bad at seperating good wines from bad. This is not a surprise given what was shown by the color plot. Almost all red wines are in one category so the clustering is clearly not differentiating based on quality of those wines. Moreover, wine ratings are inherently subjective so it's not necessarily surprising that the model was not able to differentiate poor wines from quality wines (even though good wines may share certain chemical proprties).


#Market Segmentation

```{r, echo = FALSE}
socialdf = read.csv("https://raw.githubusercontent.com/jgscott/STA380/master/data/social_marketing.csv")
```

When defining a market segment, I think of users that share similar interests. With this in mind, I believe it makes sense to try and create groupings of users based on categorical interests through clustering. Doing so may allow us to isolate specific groups that share interests in certain categories.

To begin, I ran a k-means cluster analysis  (with 8 centers) on all variables except uncategorized, chatter, spam, and adult. Given the fact that spam and adult may be attributed to bots, I wanted to see how the data would be categorized without that noise. Moreover, the uncategorized and chatter categories are likely to have a significant amount of noise as they are not as clearly defined.

**Cluster Sizes:**

```{r, echo = FALSE}
##Selecting data:
library(dplyr)
socialclean = dplyr::select(socialdf, -uncategorized, -spam, -adult, -chatter)

##Running k-means
set.seed(1)
kmeans = kmeans(socialclean[,-1], centers = 8)
socialclean$cluster = kmeans$cluster

table(socialclean$cluster)
```

Through k-means, the data seems to be fairly well split across clusters. The main exception is cluster 1, which is much larger. We can see the different frequencies for each category between clusters by creating barplots for the sums of the category frequencies across users in each cluster:

```{r, echo=FALSE}

clus1 = colSums(subset(socialclean[,-1], cluster == 1))
clus2 = colSums(subset(socialclean[,-1], cluster == 2))
clus3 = colSums(subset(socialclean[,-1], cluster == 3))
clus4 = colSums(subset(socialclean[,-1], cluster == 4))
clus5 = colSums(subset(socialclean[,-1], cluster == 5))
clus6 = colSums(subset(socialclean[,-1], cluster == 6))
clus7 = colSums(subset(socialclean[,-1], cluster == 7))
clus8 = colSums(subset(socialclean[,-1], cluster == 8))
par(mfrow=c(1,2))
barplot(clus1[1:32], main = "Cluster 1")
barplot(clus2[1:32], main = "Cluster 2")
barplot(clus3[1:32], main = "Cluster 3")
barplot(clus4[1:32], main = "Cluster 4")
barplot(clus5[1:32], main = "Cluster 5")
barplot(clus6[1:32], main = "Cluster 6")
barplot(clus7[1:32], main = "Cluster 7")
barplot(clus8[1:32], main = "Cluster 8")


```

As expected, cluster 1 appears to have faired relatively poorly in grouping users based on the categories of their tweets. The category with the highest frequency for this cluster is given as the below:

**Primary Category - Cluster 1:**
```{r, echo=FALSE}
which.max(clus1)
```

It may be that this cluster represents users that share a significant number of photos regardless of the users' other categorical interests. Summing up the frequency of photo shares across clusters reveals that cluster 1 is indeed the primary "photo sharing" cluster. 

**Photo Sharing Freq. Across Clusters:**
```{r, echo = FALSE}
library(plyr)
ddply(socialclean, .(cluster), summarise, sum = sum(photo_sharing))

```


There are several clusters within the analysis that seem to be more defined by similar interests. Namely, clusters 4, 5, 6, and 8 appear to have very strong interest in one category as well as a lesser secondary interest. Clusters 3 and 7 also have seem focused on a primary category but these clusters appear to have more than one main secondary category. Based on this, I pulled a select number of top categories for these clusters below.

**Top categories - Cluster 3 - "Young Women (?)":**

```{r, echo = FALSE}
x = order(clus3[1:32], decreasing = TRUE)
colnames(socialclean)[x[1:3]+1]
```

**Top categories - Cluster 4 - "Health Nuts":**

```{r, echo=FALSE}
x = order(clus4[1:32], decreasing = TRUE)
colnames(socialclean)[x[1:2]+1]
```

**Top categories - Cluster 5 - "Potically Minded Professionals (?)":**

```{r, echo = FALSE}
x = order(clus5[1:32], decreasing = TRUE)
colnames(socialclean)[x[1:2]+1]
```

**Top categories - Cluster 6 - "College Students":**

```{r, echo = FALSE}
x = order(clus6[1:32], decreasing = TRUE)
colnames(socialclean)[x[1:2]+1]
```

**Top categories - Cluster 7 - "Informed Adults (?)":**

```{r, echo = FALSE}
x = order(clus7[1:32], decreasing = TRUE)
colnames(socialclean)[x[1:3]+1]
```

**Top categories - Cluster 8 - "Health Nuts 2":**

```{r, echo = FALSE}
x = order(clus8[1:32], decreasing = TRUE)
colnames(socialclean)[x[1:2]+1]
```


There is clear segmentation going on between various clusters. Cluster 4 and 8 share the same primary and secondary categories and, looking back at the barplots, have very similar frequencies across all categories. These "Health Nut" clusters could be grouped into one segment for marketing and research purposes.

Moreover, the other clusters seem to be provide coherent groupings. At this point, while it may not be clear exactly what segment of the population these clusters represent (i.e. the "Young Women" cluster is named such based on my subjective experiences in the real world but they may not be in the actual cluster composition)--the analysis did succeed in finding populations with similar interests and the groupings would be very valuable for NutrientH20 in their marketing efforts.


